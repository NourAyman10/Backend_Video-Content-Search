{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8157562,"sourceType":"datasetVersion","datasetId":4825765},{"sourceId":8157625,"sourceType":"datasetVersion","datasetId":4825815},{"sourceId":8157661,"sourceType":"datasetVersion","datasetId":4825848},{"sourceId":8159539,"sourceType":"datasetVersion","datasetId":4825372},{"sourceId":8183877,"sourceType":"datasetVersion","datasetId":4831063}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%cd /kaggle/input/refvos-yt/refvos-ytvos","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:57:04.073667Z","iopub.execute_input":"2024-04-22T19:57:04.074026Z","iopub.status.idle":"2024-04-22T19:57:04.097751Z","shell.execute_reply.started":"2024-04-22T19:57:04.073995Z","shell.execute_reply":"2024-04-22T19:57:04.09679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.38.2\n!pip install torch==2.2.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121\n!pip install torchvision==0.17.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:57:04.099204Z","iopub.execute_input":"2024-04-22T19:57:04.099473Z","iopub.status.idle":"2024-04-22T20:00:05.579603Z","shell.execute_reply.started":"2024-04-22T19:57:04.099449Z","shell.execute_reply":"2024-04-22T20:00:05.578496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install yt-dlp -q -U","metadata":{"id":"f51LiJm4FKGT","outputId":"244bf4d5-02ce-4868-849a-e3f06f88d24a","execution":{"iopub.status.busy":"2024-04-22T20:00:05.580992Z","iopub.execute_input":"2024-04-22T20:00:05.581295Z","iopub.status.idle":"2024-04-22T20:00:21.597962Z","shell.execute_reply.started":"2024-04-22T20:00:05.581267Z","shell.execute_reply":"2024-04-22T20:00:21.596719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:00:21.600374Z","iopub.execute_input":"2024-04-22T20:00:21.600703Z","iopub.status.idle":"2024-04-22T20:00:25.114227Z","shell.execute_reply.started":"2024-04-22T20:00:21.600675Z","shell.execute_reply":"2024-04-22T20:00:25.113309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel, BertTokenizer\nfrom lib import segmentation\nfrom PIL import Image\nimport numpy as np\nimport os\nimport cv2\nfrom torchvision import transforms as T\nimport time\nimport datetime\nfrom scipy.signal import argrelextrema","metadata":{"id":"2Hk-BqwfXlPX","outputId":"14ec5c26-171b-4a71-c7cb-2f27d764fadd","execution":{"iopub.status.busy":"2024-04-22T20:00:25.115492Z","iopub.execute_input":"2024-04-22T20:00:25.115952Z","iopub.status.idle":"2024-04-22T20:00:28.552207Z","shell.execute_reply.started":"2024-04-22T20:00:25.115925Z","shell.execute_reply":"2024-04-22T20:00:28.55142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess\n# def download_video(video_link, video_path):\n#   # Construct the command with the video link and specify the output format as mp4\n#   command = f\"yt-dlp -f 22 -o {video_path} {video_link} \"    # Execute the command to download the video\n#   subprocess.run(command, shell=True)\n\ndef download_video(video_link):\n    # Construct the command with the video link and specify the output format as mp4\n    command = fr\"yt-dlp {video_link} -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/bestvideo+bestaudio' -o '/kaggle/working/input_video.%(ext)s'\"\n\n    # Execute the command to download the video\n    subprocess.run(command, shell=True)","metadata":{"id":"s-718BzJlO34","execution":{"iopub.status.busy":"2024-04-22T20:00:28.553275Z","iopub.execute_input":"2024-04-22T20:00:28.553695Z","iopub.status.idle":"2024-04-22T20:00:28.560407Z","shell.execute_reply.started":"2024-04-22T20:00:28.553669Z","shell.execute_reply":"2024-04-22T20:00:28.558977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smooth(x, window_len=13, window='hanning'):\n    \"\"\"smooth the data using a window with requested size.\n\n    This method is based on the convolution of a scaled window with the signal.\n    The signal is prepared by introducing reflected copies of the signal\n    (with the window size) in both ends so that transient parts are minimized\n    in the begining and end part of the output signal.\n\n    input:\n        x: the input signal\n        window_len: the dimension of the smoothing window\n        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n            flat window will produce a moving average smoothing.\n\n    output:\n        the smoothed signal\n\n    example:\n\n    import numpy as np\n    t = np.linspace(-2,2,0.1)\n    x = np.sin(t)+np.random.randn(len(t))*0.1\n    y = smooth(x)\n\n    see also:\n\n    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n    scipy.signal.lfilter\n\n    TODO: the window parameter could be the window itself if an array instead of a string\n    \"\"\"\n    if x.ndim != 1:\n        raise ValueError (\"smooth only accepts 1 dimension arrays.\")\n\n    if x.size < window_len:\n        return x\n\n    if window_len < 3:\n        return x\n\n    # if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n    #     raise ValueError (\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n\n    s = np.r_[2 * x[0] - x[window_len:1:-1],\n              x, 2 * x[-1] - x[-1:-window_len:-1]]\n    #print(len(s))\n\n    if window == 'flat':  # moving average\n        w = np.ones(window_len, 'd')\n    else:\n        w = getattr(np, window)(window_len)\n    y = np.convolve(w / w.sum(), s, mode='same')\n    return y[window_len - 1:-window_len + 1]","metadata":{"id":"VOJP_s-_SKmt","execution":{"iopub.status.busy":"2024-04-22T20:00:28.561665Z","iopub.execute_input":"2024-04-22T20:00:28.561944Z","iopub.status.idle":"2024-04-22T20:00:28.581142Z","shell.execute_reply.started":"2024-04-22T20:00:28.56192Z","shell.execute_reply":"2024-04-22T20:00:28.580255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Frame:\n    def __init__(self, id, frame):\n        self.id = id\n        self.frame = frame\ndef extract_key_frames(video_path):\n  cap = cv2.VideoCapture(video_path)\n  fps = int(cap.get(cv2.CAP_PROP_FPS))\n  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n  total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n  curr_frame = None\n  prev_frame = None\n\n  frame_diffs = []\n  frames = []\n  ret, frame = cap.read()\n  i = 1\n\n  while(ret):\n      luv = cv2.cvtColor(frame, cv2.COLOR_BGR2LUV)\n      curr_frame = luv\n      if curr_frame is not None and prev_frame is not None:\n          #logic here\n          diff = cv2.absdiff(curr_frame, prev_frame)\n          count = np.sum(diff)\n          frame_diffs.append(count)\n          frame = Frame(i-1, frame)\n          frames.append(frame)\n      prev_frame = curr_frame\n      i = i + 1\n      ret, frame = cap.read()\n  cap.release()\n\n  diff_array = np.array(frame_diffs)\n  sm_diff_array = smooth(diff_array, 5)\n  frame_indexes = np.asarray(argrelextrema(sm_diff_array, np.greater))[0]\n  keyframes = []\n  for i in frame_indexes:\n    keyframes.append(frames[i - 1])\n  print(\"# of frames : \", total_frames)\n  print(\"# of keyframes : \", len(keyframes))\n  del diff_array, sm_diff_array, frame_indexes\n  return keyframes, fps, width, height, total_frames","metadata":{"id":"AQ7JRDzCoTSl","execution":{"iopub.status.busy":"2024-04-22T20:00:28.582449Z","iopub.execute_input":"2024-04-22T20:00:28.582833Z","iopub.status.idle":"2024-04-22T20:00:28.59708Z","shell.execute_reply.started":"2024-04-22T20:00:28.582798Z","shell.execute_reply":"2024-04-22T20:00:28.596303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_emb(tokenizer, exp):\n  max_tokens = 10\n  exp = \" \".join(exp.lower().split())\n  input_ids = tokenizer.encode(text=exp, add_special_tokens=True)\n  input_ids = input_ids[:max_tokens]\n  attention_mask = [0] * max_tokens\n  padded_input_ids = [0] * max_tokens\n  padded_input_ids[:len(input_ids)] = input_ids\n  attention_mask[:len(input_ids)] = [1] * len(input_ids)\n  emb = torch.tensor(padded_input_ids).unsqueeze(0)\n  atten = torch.tensor(attention_mask).unsqueeze(0)\n  return emb, atten","metadata":{"id":"TuA_-CLbVCD9","execution":{"iopub.status.busy":"2024-04-22T20:00:28.598397Z","iopub.execute_input":"2024-04-22T20:00:28.598803Z","iopub.status.idle":"2024-04-22T20:00:28.61107Z","shell.execute_reply.started":"2024-04-22T20:00:28.598768Z","shell.execute_reply":"2024-04-22T20:00:28.610252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(keyframes, fps, width, height, total_frames, model, bert_model, emb, atten, transformer, device, output_path):\n  model.eval()\n  with torch.no_grad():\n    emb = emb.squeeze(1)\n    atten = atten.squeeze(1)\n    emb = emb.unsqueeze(-1)\n    atten = atten.unsqueeze(-1)\n    emb, atten = emb.to(device), atten.to(device)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    i = 0\n    ff = None\n    for f in keyframes:\n      ff = f.frame\n      fid = f.id\n      transformed_frame = transformer(ff)\n      last_hidden_states = bert_model(emb[:, :, 0], attention_mask=atten[:, :, 0])[0]\n      embedding = last_hidden_states[:, 0, :]\n      transformed_frame = torch.unsqueeze(transformed_frame, dim=0)\n      transformed_frame = transformed_frame.to(device)\n      output,_, _ = model(transformed_frame, embedding)\n      output = output['out'].cpu()\n      m = output.argmax(1).data.numpy()\n      m = m.squeeze(0)\n      color = np.array((0, 255, 0))\n      mask = m.reshape(m.shape[0], m.shape[1]).astype('uint8')\n      m = m > 0.5\n      ff[m] = ff[m] * 0.5 + color * 0.5\n      while(i <= fid):\n        out.write(ff)\n        i = i + 1\n    while(i< total_frames):\n      out.write(ff)\n      i = i + 1\n    out.release()","metadata":{"id":"shcge1bjZALd","execution":{"iopub.status.busy":"2024-04-22T20:00:28.614316Z","iopub.execute_input":"2024-04-22T20:00:28.614672Z","iopub.status.idle":"2024-04-22T20:00:28.626293Z","shell.execute_reply.started":"2024-04-22T20:00:28.614646Z","shell.execute_reply":"2024-04-22T20:00:28.625362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform():\n  transforms = []\n  # transforms.append(T.Resize(480, interpolation=Image.BILINEAR))\n  transforms.append(T.ToTensor())\n  transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225]))\n  return T.Compose(transforms)","metadata":{"id":"iZ2vRrFCeso8","execution":{"iopub.status.busy":"2024-04-22T20:00:28.627491Z","iopub.execute_input":"2024-04-22T20:00:28.627851Z","iopub.status.idle":"2024-04-22T20:00:28.642198Z","shell.execute_reply.started":"2024-04-22T20:00:28.627826Z","shell.execute_reply":"2024-04-22T20:00:28.641322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main(video_path, text_input, output_path):\n#   start_time = time.time()\n#   download_video(video_url, video_path)\n#   total_time = time.time() - start_time\n#   total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n#   print('{} Total time: {}'.format('downloading video', total_time_str))\n\n  start_time = time.time()\n  keyframes, fps, width, height, total_frames = extract_key_frames(video_path)\n  total_time = time.time() - start_time\n  total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n  print('{} Total time: {}'.format('keyframing', total_time_str))\n\n  start_time = time.time()\n  from args import get_parser\n  parser = get_parser()\n  s = '--resume ./checkpoints/model_davis.pth'\n  args = parser.parse_args(s.split())\n  device = torch.device(args.device)\n  model = segmentation.__dict__[args.model](num_classes=2,\n      aux_loss=False,\n      pretrained=False,\n      args=args)\n\n  model.to(device)\n  model_class = BertModel\n  bert_model = model_class.from_pretrained(args.ck_bert)\n  bert_model.to(device)\n  checkpoint = torch.load(args.resume, map_location='cpu')\n  bert_model.load_state_dict(checkpoint['bert_model'])\n  model.load_state_dict(checkpoint['model'])\n  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n  total_time = time.time() - start_time\n  total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n  print('{} Total time: {}'.format('loading models', total_time_str))\n\n  start_time = time.time()\n  transformer = get_transform()\n  emb, atten = text_to_emb(tokenizer, text_input)\n  evaluate(keyframes, fps, width, height, total_frames, model, bert_model, emb, atten, transformer, args.device, output_path)\n  total_time = time.time() - start_time\n  total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n  print('{} Total time: {}'.format('searching', total_time_str))","metadata":{"id":"TlSgXsxvqG-F","execution":{"iopub.status.busy":"2024-04-22T20:00:28.643467Z","iopub.execute_input":"2024-04-22T20:00:28.64383Z","iopub.status.idle":"2024-04-22T20:00:28.655581Z","shell.execute_reply.started":"2024-04-22T20:00:28.643798Z","shell.execute_reply":"2024-04-22T20:00:28.654736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nf = open('/kaggle/input/youtube-links/video_link.json')\ndata = json.load(f)\nvideo_link = data['link']\nquery = data['query']\nf.close()\n\ndownload_video(video_link)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:59.049572Z","iopub.execute_input":"2024-04-22T20:12:59.049934Z","iopub.status.idle":"2024-04-22T20:12:59.056258Z","shell.execute_reply.started":"2024-04-22T20:12:59.049904Z","shell.execute_reply":"2024-04-22T20:12:59.055091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main(\"/kaggle/working/input_video.mp4\", query, \"/kaggle/working/output_video.mp4\")","metadata":{"id":"IAkep9ClzhV4","outputId":"9766bc1a-37f5-438e-ae18-29257e5a8ebd","execution":{"iopub.status.busy":"2024-04-22T20:00:28.6721Z","iopub.status.idle":"2024-04-22T20:00:28.672437Z","shell.execute_reply.started":"2024-04-22T20:00:28.672272Z","shell.execute_reply":"2024-04-22T20:00:28.672286Z"},"trusted":true},"execution_count":null,"outputs":[]}]}